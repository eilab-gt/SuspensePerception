
model:
  name: 'meta-llama/Llama-2-7b-chat-hf'
  max_tokens: 100
  temperature: 1
  top_k: 50
  top_p: 0.7
  repetition_penalty: 1

experiment:
  story_file: 'gerrig'
  output_dir: 'results'
