{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = glob.glob(\"outputs/**/*.csv\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "brewer_sources = [\"paper/exp1\", \"e3\"]\n",
    "other_sources = [\"e1\", \"e2\", \"e3\"]\n",
    "experiments = [\"brewer\", \"delatorre\", \"gerrig\", \"lehne\"]\n",
    "ROOT = os.path.dirname(\"outputs/\")\n",
    "\n",
    "LIKERT_BOUNDS = {\n",
    "    \"delatorre\": [1, 9],\n",
    "    \"brewer\": [1, 7],\n",
    "    \"gerrig\": [1, 7],\n",
    "    \"lehne\": [1, 10]\n",
    "}\n",
    "\n",
    "def normalize_scalar(n, experiment):\n",
    "    lmin, lmax = LIKERT_BOUNDS[experiment]\n",
    "    normalized = (n - lmin) / (lmax - lmin)\n",
    "    return normalized * 10\n",
    "\n",
    "def get_experiment_name(experiment, runs):\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    data_blocks = glob.glob(f\"outputs/{experiment}_experiment/final/**/results.csv\", recursive=True)\n",
    "\n",
    "    likert_midpoint = (LIKERT_BOUNDS[experiment][1] - LIKERT_BOUNDS[experiment][0]) / 2\n",
    "\n",
    "    sign = lambda x: 1 if x > likert_midpoint else (0 if x < likert_midpoint else float('nan'))\n",
    "\n",
    "    keys = None\n",
    "    for block_path in data_blocks:\n",
    "\n",
    "        mat = re.search(r\"outputs\\/(.+)_experiment\\/final\\/(.+)\\/(.+)\\/(.+)\\/results.csv\", block_path)\n",
    "        model = mat.group(3)\n",
    "        if mat.group(2) not in runs:\n",
    "            continue\n",
    "        block = pd.read_csv(block_path)\n",
    "        block['model'] = model\n",
    "        block['run'] = mat.group(2)\n",
    "\n",
    "        if experiment == \"brewer\":\n",
    "            block = block[block['version'].str.contains(\"chunks\", case=False)]\n",
    "\n",
    "        try:\n",
    "            responses = block['response'].apply(ast.literal_eval)\n",
    "        except:\n",
    "            continue\n",
    "        responses = responses.apply(pd.Series)\n",
    "        if experiment == \"brewer\":\n",
    "            brewer_responses = pd.DataFrame(columns=[\"0\", \"3\", \"6\", \"9\", \"12\"])\t\n",
    "            responses = responses[[col for col in [\"0\", \"3\", \"6\", \"9\", \"12\"] if col in responses.columns]]\n",
    "            responses = pd.concat([brewer_responses, responses], axis=0)\n",
    "        block = pd.concat([block, responses], axis=1).drop(columns=['response'])\n",
    "        out = pd.concat([out, block], axis=0)\n",
    "\n",
    "        keys = responses.columns\n",
    "\n",
    "    out['id'] = out['experiment_name'] + \",\" + out['version']\n",
    "\n",
    "    for key in keys:\n",
    "        out[key] = out[key].astype(float)\n",
    "\n",
    "    out = out.drop(columns=['experiment_name', 'version'])\n",
    "\n",
    "    out = out.groupby(['id', 'model']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    out['response'] = out[keys].apply(lambda x: x.to_list(), axis=1)\n",
    "    out.drop(columns=keys, inplace=True)\n",
    "\n",
    "    out['response'] = out['response'].apply(lambda x: [normalize_scalar(n, experiment) for n in x])\n",
    "\n",
    "    if experiment == \"gerrig\":\n",
    "        out['response'] = out['response'].apply(lambda x: [x[1]])\n",
    "    if experiment == \"brewer\":\n",
    "        def reform_id(id):\n",
    "            return id.split(\",\")[0].split(\" Chunks\")[0] + \",\" + id.split(\",\")[1].split(\" Chunks\")[0]\n",
    "        out['id'] = out['id'].apply(reform_id)\n",
    "\n",
    "    return out[['id', 'model', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lehne = {\n",
    "    'Experiment,Normal': [5.565217391, 5, 4.826086957, 5.739130435, 5.52173913, 6.826086957, 7.304347826, 5.434782609, 6.391304348, 7.47826087, 7.043478261, 5.869565217, 6.739130435, 6.956521739, 6.47826087, 5.956521739, 4.652173913, 4.260869565, 5.173913043, 4.086956522, 4.173913043, 4.304347826, 5, 4.043478261, 4.217391304, 4.434782609, 5.347826087, 6.217391304, 5.434782609, 4.782608696, 6.173913043, 5.956521739, 6.47826087, 5, 4.739130435, 5.173913043, 6.304347826, 6.434782609, 5.260869565, 5.304347826, 5.956521739, 4.304347826, 5.260869565, 4.391304348, 4.956521739, 5.695652174, 5.043478261, 5.826086957, 5.043478261, 4.913043478, 5.217391304, 6.217391304, 6.391304348, 6.52173913, 7.217391304, 6.565217391, 5.52173913, 4.347826087, 3.869565217, 7, 7.565217391, 6.52173913, 6.260869565, 6.043478261, 4.913043478]\n",
    "}\n",
    "\n",
    "# Scuffed, but it works\n",
    "delatorre_global_ratings = [3.34, 3.725, 3.705, 3.89, 4.08, 5.02, 4.87, 4.81, 5.84, 5.77, 6.44, 4.685]\n",
    "delatorre_unique_categories = ['Experiment,Journalistic Bad Not Revealed',\n",
    "       'Experiment,Journalistic Bad Revealed',\n",
    "       'Experiment,Journalistic Good Not Revealed',\n",
    "       'Experiment,Journalistic Good Revealed',\n",
    "       'Experiment,Novel Bad Not Revealed',\n",
    "       'Experiment,Novel Bad Revealed',\n",
    "       'Experiment,Novel Good Not Revealed',\n",
    "       'Experiment,Novel Good Revealed']\n",
    "\n",
    "delatorre = {\n",
    "    k: delatorre_global_ratings for k in delatorre_unique_categories\n",
    "}\n",
    "\n",
    "brewer = {\n",
    "    'Experiment A,American Story Birthday' : 3.2,\n",
    "    'Experiment A,American Story Flying' : 3.6,\n",
    "    'Experiment A,American Story Lottery' : 4.5,\n",
    "    'Experiment A,American Story Old Phoebe' : 3.4,\n",
    "    'Experiment A,American Story Ylla' : 5.1,\n",
    "}\n",
    "\n",
    "gerrig = { # Standard suspense / Q2 ratings\n",
    "    \"Experiment A,Pen Not Mentioned\": (3.78 + 3.43) / 2,\n",
    "    \"Experiment A,Pen Mentioned Removed\": (4.38 + 4.06) / 2,\n",
    "    \"Experiment A,Pen Mentioned Not Removed\": 3.47,\n",
    "    \"Experiment B,Unused Comb\": 3.96,\n",
    "    \"Experiment B,Used Comb\": 3.41,\n",
    "    \"Experiment C,Prior Solution Not Mentioned\": (3.76 + 3.34) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned and Removed\": (4.61 + 3.99) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned Not Removed\": 4.14\n",
    "}\n",
    "\n",
    "def normalize_human_ratings(experiment):\n",
    "    exp_dict = globals()[experiment]\n",
    "    for key in exp_dict.keys():\n",
    "        if type(exp_dict[key]) == list:\n",
    "            exp_dict[key] = [normalize_scalar(n, experiment) for n in exp_dict[key]]\n",
    "        else:\n",
    "            exp_dict[key] = [normalize_scalar(exp_dict[key], experiment)]\n",
    "    return exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerrig_human_ratings = normalize_human_ratings(\"gerrig\")\n",
    "brewer_human_ratings = normalize_human_ratings(\"brewer\")\n",
    "lehne_human_ratings = normalize_human_ratings(\"lehne\")\n",
    "delatorre_human_ratings = normalize_human_ratings(\"delatorre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "\n",
    "gerrig_experiment_data = get_experiment_name(\"gerrig\", other_sources)\n",
    "brewer_experiment_data = get_experiment_name(\"brewer\", brewer_sources)\n",
    "lehne_experiment_data = get_experiment_name(\"lehne\", other_sources)\n",
    "delatorre_experiment_data = get_experiment_name(\"delatorre\", other_sources)\n",
    "\n",
    "def get_metrics(experiment_data, human_ratings, experiment_name, fn):\n",
    "    for row in experiment_data.iterrows():\n",
    "        ground_truth = human_ratings[row[1]['id']]\n",
    "        predictions = row[1]['response']\n",
    "\n",
    "        if len(ground_truth) == 1 and len(predictions) > 1:\n",
    "            ground_truth = ground_truth * len(predictions)\n",
    "\n",
    "        value = fn(predictions, ground_truth)\n",
    "    \n",
    "\n",
    "        # print(f\"Valid: {len([x for x in predictions if not pd.isna(x)])}, Invalid: {len([x for x in predictions if pd.isna(x)])}\")\n",
    "        experiment_data.at[row[0], 'metric'] = value\n",
    "        experiment_data.at[row[0], 'num_valid'] = len([x for x in predictions if not pd.isna(x)])\n",
    "        experiment_data.at[row[0], 'num_invalid'] = len([x for x in predictions if pd.isna(x)])\n",
    "\n",
    "    metrics_summary = experiment_data.groupby(['model']).agg({\n",
    "        'metric': ['mean', 'std'],\n",
    "        'num_valid': ['sum'],\n",
    "        'num_invalid': ['sum']\n",
    "    })\n",
    "\n",
    "    # Add a summary row with means\n",
    "    metrics_summary.loc['Average'] = metrics_summary.mean()\n",
    "\n",
    "    return metrics_summary\n",
    "\n",
    "    # # predictions = experiment_data.iloc[:, -len(value_cols):]\n",
    "    # print(predictions, human_ratings)\n",
    "    # conf_matrices = predictions.apply(lambda row: confusion_matrix(human_ratings, row, labels=[0, 1]), axis=1)\n",
    "\n",
    "def mse(x, y):\n",
    "    all_na = True\n",
    "    for i in x:\n",
    "        if not pd.isna(i):\n",
    "            all_na = False\n",
    "            break\n",
    "    if all_na:\n",
    "        return float('nan')\n",
    "\n",
    "    total = 0\n",
    "    for i in range(len(x)):\n",
    "        if pd.isna(x[i]):\n",
    "            continue\n",
    "        total += (x[i] - y[i]) ** 2\n",
    "    return total / len([x for x in x if not pd.isna(x)])    \n",
    "\n",
    "def rmse(x, y):\n",
    "    return np.sqrt(mse(x, y))\n",
    "\n",
    "def l1(x, y):\n",
    "    all_na = True\n",
    "    for i in x:\n",
    "        if not pd.isna(i):\n",
    "            all_na = False\n",
    "            break\n",
    "    if all_na:\n",
    "        return float('nan')\n",
    "\n",
    "    total = 0\n",
    "    for i in range(len(x)):\n",
    "        if pd.isna(x[i]):\n",
    "            continue\n",
    "        total += abs(x[i] - y[i])\n",
    "    return total / len([x for x in x if not pd.isna(x)])\n",
    "\n",
    "def all_metrics(fn):\n",
    "    brewer_metrics = get_metrics(brewer_experiment_data, brewer_human_ratings, \"brewer\", fn)\n",
    "    gerrig_metrics = get_metrics(gerrig_experiment_data, gerrig_human_ratings, \"gerrig\", fn)\n",
    "    delatorre_metrics = get_metrics(delatorre_experiment_data, delatorre_human_ratings, \"delatorre\", fn)\n",
    "    lehne_metrics = get_metrics(lehne_experiment_data, lehne_human_ratings, \"lehne\", fn)\n",
    "    return brewer_metrics, gerrig_metrics, delatorre_metrics, lehne_metrics\n",
    "\n",
    "brewer_metrics, gerrig_metrics, delatorre_metrics, lehne_metrics = all_metrics(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name(model : str):\n",
    "    excluded = [\"Average\", \"Consensus\"]\n",
    "    if model in excluded:\n",
    "        return model\n",
    "    model_name = model.split(\"_\")[-1]\n",
    "    model_name = model_name.split(\"-Instruct\")[0]\n",
    "    model_name = model_name.split(\"-chat\")[0]\n",
    "    model_name = model_name.split(\"-it\")[0]\n",
    "    model_name = model_name.replace(\"-\", \" \")\n",
    "    model_name = model_name[0].upper() + model_name[1:]\n",
    "    return model_name\n",
    "\n",
    "def prettify_table(table):\n",
    "    table = table.copy()\n",
    "    numerics = ['metric']\n",
    "    def format_mean_std(x):\n",
    "        if pd.isna(x['std']):\n",
    "            return f\"{x['mean']:.2f} ± 0.0\"\n",
    "        return f\"{x['mean']:.2f} ± {x['std']:.2f}\"\n",
    "    for metric in numerics:\n",
    "        table[metric, 'mean'] = table[metric, 'mean'].apply(lambda x: round(x, 2))\n",
    "        table[metric, 'std'] = table[metric, 'std'].apply(lambda x: round(x, 2))\n",
    "        collapsed = table[metric].apply(format_mean_std, axis=1)\n",
    "        table[metric] = collapsed\n",
    "        table.drop(columns=[(metric, 'std')], inplace=True)\n",
    "    table.columns = table.columns.droplevel(1)\n",
    "    table = table.reset_index()\n",
    "    table['model'] = table['model'].apply(format_model_name)\n",
    "\n",
    "    table.set_index('model', inplace=True)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_gerrig = prettify_table(gerrig_metrics)\n",
    "pretty_brewer = prettify_table(brewer_metrics)\n",
    "pretty_delatorre = prettify_table(delatorre_metrics)\n",
    "pretty_lehne = prettify_table(lehne_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>num_valid</th>\n",
       "      <th>num_invalid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen2 72B</th>\n",
       "      <td>1.79 ± 0.50</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek V3</th>\n",
       "      <td>2.44 ± 0.77</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 27b</th>\n",
       "      <td>1.98 ± 0.54</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9b</th>\n",
       "      <td>1.59 ± 0.83</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 2 7b</th>\n",
       "      <td>3.94 ± 1.82</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3 70b</th>\n",
       "      <td>1.77 ± 0.90</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3 8b</th>\n",
       "      <td>1.26 ± 0.34</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WizardLM 2 8x22B</th>\n",
       "      <td>2.15 ± 1.26</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B</th>\n",
       "      <td>1.03 ± 0.22</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral 8x7B</th>\n",
       "      <td>2.40 ± 1.36</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>2.03 ± 0.85</td>\n",
       "      <td>23.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       metric  num_valid  num_invalid\n",
       "model                                                \n",
       "Qwen2 72B         1.79 ± 0.50       25.0          0.0\n",
       "DeepSeek V3       2.44 ± 0.77       25.0          0.0\n",
       "Gemma 2 27b       1.98 ± 0.54       25.0          0.0\n",
       "Gemma 2 9b        1.59 ± 0.83       25.0          0.0\n",
       "Llama 2 7b        3.94 ± 1.82       16.0          9.0\n",
       "Llama 3 70b       1.77 ± 0.90       25.0          0.0\n",
       "Llama 3 8b        1.26 ± 0.34       25.0          0.0\n",
       "WizardLM 2 8x22B  2.15 ± 1.26       25.0          0.0\n",
       "Mistral 7B        1.03 ± 0.22       25.0          0.0\n",
       "Mixtral 8x7B      2.40 ± 1.36       23.0          2.0\n",
       "Average           2.03 ± 0.85       23.9          1.1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_brewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_single_metric(tables : list[pd.DataFrame], metric : str, labels : list[str] = None):\n",
    "    out = pd.concat(tables, axis=1)\n",
    "    out = out[[metric]]\n",
    "    if labels:\n",
    "        out.columns = labels\n",
    "    out = out.reset_index()\n",
    "    for col in out.columns:\n",
    "        col = col.upper()\n",
    "\n",
    "    return out.reset_index()\n",
    "\n",
    "def as_latex(table):\n",
    "    table = table.drop(columns=['index'])\n",
    "    table = table.rename(columns={'model': 'Model'})\n",
    "    return table.to_latex(index=False, escape=False)\n",
    "\n",
    "joined = join_with_single_metric([pretty_gerrig, pretty_brewer, pretty_delatorre, pretty_lehne], 'metric', labels=['Gerrig', 'Brewer', 'Delatorre', 'Lehne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model & Gerrig & Brewer & Delatorre & Lehne \\\\\n",
      "\\midrule\n",
      "Qwen2 72B & 3.61 ± 0.60 & 1.79 ± 0.50 & 2.60 ± 0.32 & 1.88 ± 0.0 \\\\\n",
      "DeepSeek V3 & 3.61 ± 0.60 & 2.44 ± 0.77 & 2.21 ± 0.16 & 1.73 ± 0.0 \\\\\n",
      "Gemma 2 27b & 3.41 ± 0.70 & 1.98 ± 0.54 & 1.97 ± 0.17 & 2.07 ± 0.0 \\\\\n",
      "Gemma 2 9b & 2.99 ± 1.11 & 1.59 ± 0.83 & 2.36 ± 0.36 & 2.12 ± 0.0 \\\\\n",
      "Llama 2 7b & 3.61 ± 0.60 & 3.94 ± 1.82 & 3.22 ± 0.22 & 3.31 ± 0.0 \\\\\n",
      "Llama 3 70b & 3.61 ± 0.60 & 1.77 ± 0.90 & 3.15 ± 0.23 & 2.54 ± 0.0 \\\\\n",
      "Llama 3 8b & 5.07 ± 0.70 & 1.26 ± 0.34 & 2.87 ± 0.19 & 2.66 ± 0.0 \\\\\n",
      "WizardLM 2 8x22B & 3.61 ± 0.60 & 2.15 ± 1.26 & 2.46 ± 0.67 & 1.75 ± 0.0 \\\\\n",
      "Mistral 7B & 2.64 ± 0.99 & 1.03 ± 0.22 & 2.59 ± 0.52 & 3.00 ± 0.0 \\\\\n",
      "Mixtral 8x7B & 2.71 ± 1.32 & 2.40 ± 1.36 & 2.86 ± 0.42 & 2.04 ± 0.0 \\\\\n",
      "Average & 3.49 ± 0.78 & 2.03 ± 0.85 & 2.63 ± 0.33 & 2.31 ± 0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(as_latex(joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>response</th>\n",
       "      <th>metric</th>\n",
       "      <th>num_valid</th>\n",
       "      <th>num_invalid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>Qwen_Qwen2-72B-Instruct</td>\n",
       "      <td>[7.5, 4.166666666666666, 1.666666666666667, 5....</td>\n",
       "      <td>2.384201</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>deepseek-ai_DeepSeek-V3</td>\n",
       "      <td>[7.5, 3.75, 5.0, 6.25, 7.5, 8.75, 5.0, 2.5, 7....</td>\n",
       "      <td>2.116493</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>google_gemma-2-27b-it</td>\n",
       "      <td>[5.0, 2.916666666666667, 3.333333333333333, 6....</td>\n",
       "      <td>2.206076</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>google_gemma-2-9b-it</td>\n",
       "      <td>[6.25, 3.75, 5.0, 7.5, 6.25, 9.166666666666668...</td>\n",
       "      <td>2.672049</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>meta-llama_Llama-2-7b-chat-hf</td>\n",
       "      <td>[7.5, 5.416666666666666, 7.5, 6.25, 7.08333333...</td>\n",
       "      <td>3.072049</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>meta-llama_Llama-3-70b-chat-hf</td>\n",
       "      <td>[6.25, 2.916666666666667, 8.75, 0.833333333333...</td>\n",
       "      <td>3.065104</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>meta-llama_Llama-3-8b-chat-hf</td>\n",
       "      <td>[6.25, 5.0, 8.75, 0.4166666666666666, 5.833333...</td>\n",
       "      <td>3.261285</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>microsoft_WizardLM-2-8x22B</td>\n",
       "      <td>[7.083333333333334, 5.0, 8.333333333333334, 2....</td>\n",
       "      <td>3.014410</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>mistralai_Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>[7.5, 5.0, 7.5, 0.0, 6.25, 8.75, 8.33333333333...</td>\n",
       "      <td>3.049132</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>mistralai_Mixtral-8x7B-Instruct-v0.1</td>\n",
       "      <td>[8.333333333333334, 5.0, 7.916666666666666, 1....</td>\n",
       "      <td>3.153299</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id  \\\n",
       "0   Experiment,Journalistic Bad Not Revealed   \n",
       "1   Experiment,Journalistic Bad Not Revealed   \n",
       "2   Experiment,Journalistic Bad Not Revealed   \n",
       "3   Experiment,Journalistic Bad Not Revealed   \n",
       "4   Experiment,Journalistic Bad Not Revealed   \n",
       "..                                       ...   \n",
       "75            Experiment,Novel Good Revealed   \n",
       "76            Experiment,Novel Good Revealed   \n",
       "77            Experiment,Novel Good Revealed   \n",
       "78            Experiment,Novel Good Revealed   \n",
       "79            Experiment,Novel Good Revealed   \n",
       "\n",
       "                                   model  \\\n",
       "0                Qwen_Qwen2-72B-Instruct   \n",
       "1                deepseek-ai_DeepSeek-V3   \n",
       "2                  google_gemma-2-27b-it   \n",
       "3                   google_gemma-2-9b-it   \n",
       "4          meta-llama_Llama-2-7b-chat-hf   \n",
       "..                                   ...   \n",
       "75        meta-llama_Llama-3-70b-chat-hf   \n",
       "76         meta-llama_Llama-3-8b-chat-hf   \n",
       "77            microsoft_WizardLM-2-8x22B   \n",
       "78    mistralai_Mistral-7B-Instruct-v0.3   \n",
       "79  mistralai_Mixtral-8x7B-Instruct-v0.1   \n",
       "\n",
       "                                             response    metric  num_valid  \\\n",
       "0   [7.5, 4.166666666666666, 1.666666666666667, 5....  2.384201       12.0   \n",
       "1   [7.5, 3.75, 5.0, 6.25, 7.5, 8.75, 5.0, 2.5, 7....  2.116493       12.0   \n",
       "2   [5.0, 2.916666666666667, 3.333333333333333, 6....  2.206076       12.0   \n",
       "3   [6.25, 3.75, 5.0, 7.5, 6.25, 9.166666666666668...  2.672049       12.0   \n",
       "4   [7.5, 5.416666666666666, 7.5, 6.25, 7.08333333...  3.072049       12.0   \n",
       "..                                                ...       ...        ...   \n",
       "75  [6.25, 2.916666666666667, 8.75, 0.833333333333...  3.065104       12.0   \n",
       "76  [6.25, 5.0, 8.75, 0.4166666666666666, 5.833333...  3.261285       12.0   \n",
       "77  [7.083333333333334, 5.0, 8.333333333333334, 2....  3.014410       12.0   \n",
       "78  [7.5, 5.0, 7.5, 0.0, 6.25, 8.75, 8.33333333333...  3.049132       12.0   \n",
       "79  [8.333333333333334, 5.0, 7.916666666666666, 1....  3.153299       12.0   \n",
       "\n",
       "    num_invalid  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "..          ...  \n",
       "75          0.0  \n",
       "76          0.0  \n",
       "77          0.0  \n",
       "78          0.0  \n",
       "79          0.0  \n",
       "\n",
       "[80 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delatorre_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
