{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_llm_ratings(llm_rating_sources: list[str]) -> dict[str, dict[str, list[float]]]:\n",
    "    llm_ratings = defaultdict(lambda: defaultdict(list))  \n",
    "\n",
    "    for source in llm_rating_sources:\n",
    "        source_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "        \n",
    "        for root, _, files in os.walk(source_path):\n",
    "            model_name = os.path.basename(os.path.dirname(root))\n",
    "            \n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path, header=None, names=['experiment_name', 'version', 'response'])\n",
    "\n",
    "                    for _, row in df.iterrows():\n",
    "                        experiment_name = row[\"experiment_name\"]\n",
    "                        version = row[\"version\"]\n",
    "                        response = row[\"response\"]\n",
    "                        \n",
    "                        try:\n",
    "                            \n",
    "                            response_dict = json.loads(response.replace(\"'\", \"\\\"\"))\n",
    "                        except json.JSONDecodeError:\n",
    "                            try:\n",
    "                                response_dict = ast.literal_eval(response)\n",
    "                            except (ValueError, SyntaxError):\n",
    "\n",
    "                                continue  \n",
    "\n",
    "                        value_at_key_1 = response_dict.get('1')\n",
    "\n",
    "                        if value_at_key_1 is not None:\n",
    "                            llm_ratings[model_name][experiment_name + \", \" + version] = value_at_key_1\n",
    "\n",
    "    return llm_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_ratings = { # Standard suspense / Q2 ratings\n",
    "    \"Experiment A, Pen Not Mentioned\": (3.78 + 3.43) / 2,\n",
    "    \"Experiment A, Pen Mentioned Removed\": (4.38 + 4.06) / 2,\n",
    "    \"Experiment A, Pen Mentioned Not Removed\": 3.47,\n",
    "    \"Experiment B, Unused Comb\": 3.96,\n",
    "    \"Experiment B, Used Comb\": 3.41,\n",
    "    \"Experiment C, Prior Solution Not Mentioned\": (3.76 + 3.34) / 2,\n",
    "    \"Experiment C, Prior Solution Mentioned and Removed\": (4.61 + 3.99) / 2,\n",
    "    \"Experiment C, Prior Solution Mentioned Not Removed\": 4.14\n",
    "}\n",
    "# human_ratings = { # Alternate suspense / Q2 ratings\n",
    "#     \"Experiment A, Pen Not Mentioned\": 3.40,\n",
    "#     \"Experiment A, Pen Mentioned Removed\": 3.95,\n",
    "#     \"Experiment A, Pen Mentioned Not Removed\": 0, # n/a\n",
    "#     \"Experiment B, Unused Comb\": 0, # n/a\n",
    "#     \"Experiment B, Used Comb\": 0, # n/a\n",
    "#     \"Experiment C, Prior Solution Not Mentioned\": 3.40,\n",
    "#     \"Experiment C, Prior Solution Mentioned and Removed\": 3.91,\n",
    "#     \"Experiment C, Prior Solution Mentioned Not Removed\": 0 # n/a\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_llm_ratings(llm_ratings_list: list[dict]) -> dict:\n",
    "    averaged_ratings = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for llm_ratings in llm_ratings_list:\n",
    "        for model_name, experiments in llm_ratings.items():\n",
    "            for experiment_name, ratings in experiments.items():\n",
    "                averaged_ratings[model_name][experiment_name].append(ratings)\n",
    "\n",
    "    for model_name, experiments in averaged_ratings.items():\n",
    "        for experiment_name, ratings in experiments.items():\n",
    "            if ratings:\n",
    "                averaged_ratings[model_name][experiment_name] = np.mean(ratings)\n",
    "\n",
    "    return averaged_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rating_sources1 = [\"gerrig_experiment/final/e1\"]\n",
    "llm_ratings1 = get_llm_ratings(llm_rating_sources1)\n",
    "llm_rating_sources2 = [\"gerrig_experiment/final/e2\"]\n",
    "llm_ratings2 = get_llm_ratings(llm_rating_sources2)\n",
    "llm_rating_sources3 = [\"gerrig_experiment/final/e3\"]\n",
    "llm_ratings3 = get_llm_ratings(llm_rating_sources3)\n",
    "averaged_ratings = average_llm_ratings([llm_ratings1, llm_ratings2, llm_ratings3])\n",
    "print(averaged_ratings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dict = {\n",
    "    model: {\n",
    "        experiment: (ratings.tolist() if isinstance(ratings, (list, np.ndarray)) else ratings)\n",
    "        for experiment, ratings in experiments.items()\n",
    "    }\n",
    "    for model, experiments in averaged_ratings.items()\n",
    "}\n",
    "\n",
    "print(normal_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "models = list(normal_dict.keys())\n",
    "experiments = list(human_ratings.keys())\n",
    "shortened_model_names = [\"DS-V3\", \"G-27B\", \"G-9B\", 'L2-7B', 'L3-70B', 'L3-8B', 'W-22B', 'M-7B', 'Mx-7B', 'Q-72B']\n",
    "\n",
    "actual =[]\n",
    "for model in models:\n",
    "    for experiment in experiments:\n",
    "        llm_rating = normal_dict[model].get(experiment, np.nan)\n",
    "        human_rating = human_ratings.get(experiment, np.nan)\n",
    "        \n",
    "        if not np.isnan(llm_rating) and not np.isnan(human_rating):\n",
    "            proximity = 1 - abs(llm_rating - human_rating) / max(1, abs(human_rating))\n",
    "            data.append([model, experiment, llm_rating, human_rating, proximity])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Model\", \"Experiment\", \"LLM Rating\", \"Human Rating\", \"Proximity\"])\n",
    "human_row = pd.DataFrame([[\"Human\", experiment, np.nan, human_ratings[experiment], 1.0] for experiment in experiments], columns=[\"Model\", \"Experiment\", \"LLM Rating\", \"Human Rating\", \"Proximity\"])\n",
    "\n",
    "df = pd.concat([df, human_row], ignore_index=True)\n",
    "\n",
    "df_pivot = df.pivot(index=\"Model\", columns=\"Experiment\", values=\"Proximity\")\n",
    "df_pivot = df_pivot.reindex(df_pivot.index.tolist() + ['Human'])\n",
    "df_pivot = df_pivot[~df_pivot.index.duplicated(keep='last')]\n",
    "\n",
    "annot_data = df.pivot(index=\"Model\", columns=\"Experiment\", values=\"LLM Rating\").reindex(df_pivot.index)\n",
    "human_data = df.pivot(index=\"Model\", columns=\"Experiment\", values=\"Human Rating\").reindex(df_pivot.index)\n",
    "\n",
    "annot_data_rounded = annot_data.round(2)\n",
    "human_data_rounded = human_data.round(2)\n",
    "\n",
    "annot_matrix = annot_data_rounded.astype(str)\n",
    "annot_matrix.loc['Human', :] = human_data_rounded.loc['Human'].astype(str)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.heatmap(df_pivot, vmin=0, vmax=1, annot=annot_matrix, cmap=\"viridis\", fmt=\"\", linewidths=0.5, yticklabels=shortened_model_names + [\"H\"],\n",
    "            annot_kws={\"size\": 10, \"va\": \"center\", \"ha\": \"center\"})\n",
    "ax.collections[0].colorbar.ax.set_title(\"% Agreement\", fontsize=10)\n",
    "\n",
    "plt.title(f'Gerrig Average Ratings by Model', fontsize=14)\n",
    "plt.xlabel(\"Experiment\", fontsize=14)\n",
    "plt.ylabel(\"Model\", fontsize=14)\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "experiments = list(human_ratings.keys())\n",
    "llm_values1 = [llm_ratings1.get(exp, 0) for exp in experiments]\n",
    "llm_values2 = [llm_ratings2.get(exp, 0) for exp in experiments]\n",
    "llm_values3 = [llm_ratings3.get(exp, 0) for exp in experiments]\n",
    "human_values = [human_ratings.get(exp, 0) for exp in experiments]\n",
    "\n",
    "llm_means = np.mean([llm_values1, llm_values2, llm_values3], axis=0)\n",
    "llm_stds = np.std([llm_values1, llm_values2, llm_values3], axis=0)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(experiments))\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.bar(index, human_values, bar_width, label='Human Ratings')\n",
    "ax.bar(index + bar_width, llm_means, bar_width, yerr=llm_stds, capsize=5, label='LLM Ratings (Avg)', color='orange')\n",
    "ax.set_xlabel('Experiments')\n",
    "ax.set_ylabel('Suspense Ratings')\n",
    "ax.set_title('Human vs LLM Suspense Ratings')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(experiments, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "experiments = list(human_ratings.keys())\n",
    "llm_values1 = [llm_ratings1.get(exp, 0) for exp in experiments]\n",
    "llm_values2 = [llm_ratings2.get(exp, 0) for exp in experiments]\n",
    "llm_values3 = [llm_ratings3.get(exp, 0) for exp in experiments]\n",
    "human_values = [human_ratings.get(exp, 0) for exp in experiments]\n",
    "\n",
    "llm_means = np.mean([llm_values1, llm_values2, llm_values3], axis=0)\n",
    "\n",
    "differences = []\n",
    "for i in range(len(llm_means)):\n",
    "    diff_llm = abs(llm_means[i] - human_values[i])\n",
    "    similarity = 1 - (diff_llm / 7)\n",
    "    differences.append(similarity)\n",
    "heatmap_data = np.zeros((1, len(experiments)))\n",
    "for i in range(len(differences)):\n",
    "    heatmap_data[0, i] = differences[i] \n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"viridis\", cbar_kws={'label': 'Difference (Human - LLM)', 'ticks': [0, 0.5, 1]}, xticklabels=experiments, yticklabels=[\"1\"], cbar=True, vmin=0, vmax=1)\n",
    "plt.xlabel('Experiments')\n",
    "plt.ylabel('Human-LLM Agreement')\n",
    "plt.title('Agreement Between Human and LLM Suspense Ratings (1 = Agreement , 0 = Disagreement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# experiments = list(human_ratings.keys())\n",
    "# llm_values1 = [llm_ratings1.get(exp, 0) for exp in experiments]\n",
    "# llm_values2 = [llm_ratings2.get(exp, 0) for exp in experiments]\n",
    "# llm_values3 = [llm_ratings3.get(exp, 0) for exp in experiments]\n",
    "# human_values = [human_ratings.get(exp, 0) for exp in experiments]\n",
    "\n",
    "# llm_means = np.mean([llm_values1, llm_values2, llm_values3], axis=0)\n",
    "# mean = 3.5\n",
    "# differences = []\n",
    "\n",
    "# for i in range(0, len(llm_means)):\n",
    "#     diff_llm = llm_means[i] - mean\n",
    "#     diff_human = human_values[i] - mean\n",
    "    \n",
    "#     if diff_llm > 0:\n",
    "#         differences.append(1)\n",
    "#     elif diff_llm == 0:\n",
    "#         differences.append(0.5)\n",
    "#     else:\n",
    "#         differences.append(0)\n",
    "\n",
    "# heatmap_data = np.zeros((2, len(experiments))) \n",
    "# for i, diff in enumerate(differences):\n",
    "#     heatmap_data[1, i] = diff\n",
    "#     if diff_human > 0:\n",
    "#         heatmap_data[0, i] = 1\n",
    "#     elif diff_human == 0:\n",
    "#         heatmap_data[0, i] = 0.5\n",
    "#     else:\n",
    "#         heatmap_data[1, i] = 0\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar_kws={'label': 'Suspenseful or not', 'ticks': [0, 0.5, 1]}, \n",
    "#             xticklabels=experiments, yticklabels=[\"Human\", \"LLM\"], cbar=True, vmin=0, vmax=1, linewidths=0.5)\n",
    "# plt.xlabel('Experiments')\n",
    "# plt.ylabel('Human & LLM')\n",
    "# plt.title('Suspense characteristic of the Experiments')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming normal_dict, llm_means, human_values, and other variables are defined and available\n",
    "\n",
    "# Extract LLMs and experiments\n",
    "llm_names = list(llm_means.keys())  # Names of the LLMs\n",
    "experiments = list(next(iter(llm_means.values())).keys())  # Experiment names (same for all LLMs)\n",
    "\n",
    "# Initialize differences and heatmap data structure\n",
    "differences = []\n",
    "heatmap_data = np.zeros((len(llm_names) + 1, len(experiments)))  # Add 1 for Human row\n",
    "annot_data = np.empty((len(llm_names) + 1, len(experiments)), dtype=object)  # Prepare annot_data for strings\n",
    "\n",
    "# Calculate differences for each LLM\n",
    "for i, llm in enumerate(llm_names):\n",
    "    llm_experiment_values = list(llm_means[llm].values())  # Get the experiment values for this LLM\n",
    "    for j, experiment in enumerate(experiments):\n",
    "        # Ensure values are numeric before proceeding\n",
    "        try:\n",
    "            llm_value = float(llm_experiment_values[j])\n",
    "        except ValueError:\n",
    "            llm_value = 0  # Handle cases where non-numeric values exist (you can adjust this logic)\n",
    "        \n",
    "        diff_llm = abs(llm_value - human_ratings[j])  # Difference with human\n",
    "        similarity = 1 - (diff_llm / 7)  # Similarity calculation\n",
    "        differences.append(similarity)\n",
    "        heatmap_data[i, j] = similarity * 100  # Store similarity in percentage\n",
    "        \n",
    "        # Add the value for this LLM and experiment to the annot_data (original values as strings)\n",
    "        annot_data[i, j] = f\"{llm_experiment_values[j]}\"  # Store as string\n",
    "# Add human values as the last row in heatmap data\n",
    "heatmap_data[-1, :] = human_ratings\n",
    "\n",
    "# Annotate heatmap with original values (as strings)\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=annot_data, \n",
    "    fmt=\"s\",  # \"s\" ensures values are treated as strings\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kws={'label': 'Similarity Percentage', 'ticks': [0, 50, 100]},\n",
    "    xticklabels=experiments,\n",
    "    yticklabels=llm_names + [\"Human\"],  # LLM names + Human at the bottom\n",
    "    cbar=True,\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.xlabel('Experiments')\n",
    "plt.ylabel('LLM & Human')\n",
    "plt.title('Suspense Characteristic Comparison (Human vs LLMs)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
