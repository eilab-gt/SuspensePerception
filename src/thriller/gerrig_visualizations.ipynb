{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_ratings(llm_rating_sources: list[str]) -> dict[str, float]:\n",
    "    llm_ratings = {}\n",
    "\n",
    "    all_ratings = {}\n",
    "    for source in llm_rating_sources:\n",
    "        source = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "        \n",
    "        # Traverse through the directory to find CSV files\n",
    "        for root, _, files in os.walk(source):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # Read the CSV file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Assuming the response column contains the dictionary of ratings as string\n",
    "                    for index, row in df.iterrows():\n",
    "                        name = row['experiment_name'] + \", \" + row['version']\n",
    "                        response = row[\"response\"]\n",
    "\n",
    "                        response = ast.literal_eval(response)\n",
    "                        question = \"Q2\"\n",
    "                        rating = 0\n",
    "                        if question in response:\n",
    "                            rating = response[\"Q2\"]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                        if name not in all_ratings:\n",
    "                            all_ratings[name] = []\n",
    "                        all_ratings[name].append(rating)\n",
    "    \n",
    "    for key, value in all_ratings.items():\n",
    "        llm_ratings[key] = sum(value) / len(value)\n",
    "    \n",
    "    return llm_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_ratings = { # Standard suspense / Q2 ratings\n",
    "#     \"Experiment A, Pen Not Mentioned\": (3.78 + 3.43) / 2,\n",
    "#     \"Experiment A, Pen Mentioned Removed\": (4.38 + 4.06) / 2,\n",
    "#     \"Experiment A, Pen Mentioned Not Removed\": 3.47,\n",
    "#     \"Experiment B, Unused Comb\": 3.96,\n",
    "#     \"Experiment B, Used Comb\": 3.41,\n",
    "#     \"Experiment C, Prior Solution Not Mentioned\": (3.76 + 3.34) / 2,\n",
    "#     \"Experiment C, Prior Solution Mentioned and Removed\": (4.61 + 3.99) / 2,\n",
    "#     \"Experiment C, Prior Solution Mentioned Not Removed\": 4.14\n",
    "# }\n",
    "human_ratings = { # Alternate suspense / Q2 ratings\n",
    "    \"Experiment A, Pen Not Mentioned\": 3.40,\n",
    "    \"Experiment A, Pen Mentioned Removed\": 3.95,\n",
    "    \"Experiment A, Pen Mentioned Not Removed\": 0, # n/a\n",
    "    \"Experiment B, Unused Comb\": 0, # n/a\n",
    "    \"Experiment B, Used Comb\": 0, # n/a\n",
    "    \"Experiment C, Prior Solution Not Mentioned\": 3.40,\n",
    "    \"Experiment C, Prior Solution Mentioned and Removed\": 3.91,\n",
    "    \"Experiment C, Prior Solution Mentioned Not Removed\": 0 # n/a\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rating_sources = [\"gerrig_experiment/standard\"]\n",
    "\n",
    "llm_ratings = get_llm_ratings(llm_rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "experiments = list(human_ratings.keys())\n",
    "llm_values = [llm_ratings.get(exp, 0) for exp in experiments]\n",
    "human_values = [human_ratings.get(exp, 0) for exp in experiments]\n",
    "ratings_diff = [llm_value - human_value for llm_value, human_value in zip(llm_values, human_values)]\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.5\n",
    "index = np.arange(len(experiments))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bars1 = ax.bar(index, ratings_diff, bar_width, label='LLM-Human Ratings Diff')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Experiments')\n",
    "ax.set_ylabel('Suspense Rating Diff')\n",
    "ax.set_title('LLM vs Human Suspense Ratings Alternate')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(experiments, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
