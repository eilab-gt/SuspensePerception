{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_ratings(llm_rating_sources: list[str]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the average llm rating for a given source.\n",
    "    Args:\n",
    "        llm_rating_sources: list of llm rating sources. This is the dirname to start checking for csv files from.\n",
    "                            Subdirectories will be checked starting from this directory.\n",
    "    Return:\n",
    "        List of all llm ratings for the given source.\n",
    "        If multiple sources are given return the average rating across all sources.\n",
    "    \"\"\"\n",
    "    all_source_ratings = []\n",
    "\n",
    "    for source in llm_rating_sources:\n",
    "        source = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "        source_ratings = []\n",
    "        \n",
    "        # Traverse through the directory to find CSV files\n",
    "        for root, _, files in os.walk(source):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # Read the CSV file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Assuming the response column contains the dictionary of ratings as string\n",
    "                    if 'response' in df.columns:\n",
    "                        response = df['response'][0]\n",
    "\n",
    "                        # Convert the response from string to a dictionary\n",
    "                        response_dict = ast.literal_eval(response)\n",
    "                        \n",
    "                        # Extract ratings and append them to source_ratings\n",
    "                        source_ratings = response_dict.values()\n",
    "        \n",
    "        if source_ratings:\n",
    "            all_source_ratings.append(source_ratings)\n",
    "\n",
    "    # Calculate average ratings\n",
    "    zipped_lists = zip_longest(*all_source_ratings, fillvalue=None)\n",
    "    llm_ratings = [\n",
    "        sum(filter(None.__ne__, group)) / len(list(filter(None.__ne__, group)))\n",
    "        for group in zipped_lists\n",
    "    ]\n",
    "    \n",
    "    return llm_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rating_sources = [\"bentz_experiment/standard/creative\"]\n",
    "\n",
    "llm_ratings = get_llm_ratings(llm_rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HUMAN_RATINGS = len(human_ratings) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(llm_ratings, label='LLM Ratings')\n",
    "\n",
    "plt.title('LLM Suspense Ratings')\n",
    "plt.xlabel('Passage')\n",
    "plt.ylabel('Suspense Rating')\n",
    "plt.legend()\n",
    "\n",
    "if USE_HUMAN_RATINGS:\n",
    "    plt.plot(human_ratings, label='Human Ratings')\n",
    "    plt.title('LLM vs Human Suspense Ratings')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ratings_norm = (np.array(llm_ratings) - np.min(llm_ratings)) / (np.max(llm_ratings) - np.min(llm_ratings))\n",
    "plt.plot(llm_ratings_norm, label='LLM Ratings Normalized')\n",
    "\n",
    "plt.title('Normalized LLM Suspense Ratings')\n",
    "plt.xlabel('Passage')\n",
    "plt.ylabel('Suspense Rating')\n",
    "plt.legend()\n",
    "\n",
    "if USE_HUMAN_RATINGS:\n",
    "    human_ratings_norm = (np.array(human_ratings) - np.min(human_ratings)) / (np.max(human_ratings) - np.min(human_ratings))\n",
    "    plt.plot(human_ratings_norm, label='Human Ratings Normalized')\n",
    "    plt.title('Normalized LLM vs Human Suspense Ratings')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ratings_diff = np.diff(llm_ratings)\n",
    "plt.plot(llm_ratings_diff, label='LLM Ratings Diff')\n",
    "\n",
    "plt.title('LLM Suspense Ratings Diff')\n",
    "plt.xlabel('Passage')\n",
    "plt.ylabel('Suspense Rating')\n",
    "plt.legend()\n",
    "\n",
    "if USE_HUMAN_RATINGS:\n",
    "    human_ratings_diff = np.diff(human_ratings)\n",
    "    plt.plot(human_ratings_diff, label='Human Ratings Diff')\n",
    "    plt.title('LLM vs Human Suspense Ratings Diff')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
