{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = glob.glob(\"../../outputs/**/*.csv\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brewer_sources = [\"paper/exp1\", \"e3\"]\n",
    "other_sources = [\"e1\", \"e2\", \"e3\"]\n",
    "experiments = [\"brewer\", \"delatorre\", \"gerrig\", \"lehne\"]\n",
    "ROOT = os.path.dirname(\"../../outputs/\")\n",
    "\n",
    "LIKERT_BOUNDS = {\n",
    "    \"delatorre\": [1, 9],\n",
    "    \"brewer\": [1, 7],\n",
    "    \"gerrig\": [1, 7],\n",
    "    \"lehne\": [1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "def get_experiment_name(experiment, runs):\n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    data_blocks = glob.glob(f\"../../outputs/{experiment}_experiment/final/**/results.csv\", recursive=True)\n",
    "\n",
    "    likert_midpoint = (LIKERT_BOUNDS[experiment][1] - LIKERT_BOUNDS[experiment][0]) / 2\n",
    "\n",
    "    sign = lambda x: 1 if x > likert_midpoint else (0 if x < likert_midpoint else float('nan'))\n",
    "\n",
    "    keys = None\n",
    "    for block_path in data_blocks:\n",
    "        mat = re.search(r\"../../outputs[\\\\/](.+)_experiment[\\\\/]final[\\\\/](.+)[\\\\/](.+)[\\\\/](.+)[\\\\/]results.csv\", block_path)\n",
    "        model = mat.group(3)\n",
    "        if mat.group(2) not in runs:\n",
    "            continue\n",
    "        block = pd.read_csv(block_path)\n",
    "        block['model'] = model\n",
    "        block['run'] = mat.group(2)\n",
    "\n",
    "        if experiment == \"brewer\":\n",
    "            block = block[block['version'].str.contains(\"chunks\", case=False)]\n",
    "\n",
    "        try:\n",
    "            responses = block['response'].apply(ast.literal_eval)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        responses = responses.apply(pd.Series)\n",
    "        if experiment == \"brewer\":\n",
    "            brewer_responses = pd.DataFrame(columns=[\"0\", \"3\", \"6\", \"9\", \"12\"])\t\n",
    "            responses = responses[[col for col in [\"0\", \"3\", \"6\", \"9\", \"12\"] if col in responses.columns]]\n",
    "            responses = pd.concat([brewer_responses, responses], axis=0)\n",
    "        block = pd.concat([block, responses], axis=1).drop(columns=['response'])\n",
    "        out = pd.concat([out, block], axis=0)\n",
    "\n",
    "        keys = responses.columns\n",
    "\n",
    "    out['id'] = out['experiment_name'] + \",\" + out['version']\n",
    "\n",
    "    for key in keys:\n",
    "        out[key] = out[key].astype(float)\n",
    "\n",
    "    out = out.drop(columns=['experiment_name', 'version'])\n",
    "\n",
    "    out = out.groupby(['id', 'model']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    out['response'] = out[keys].apply(lambda x: x.to_list(), axis=1)\n",
    "    out.drop(columns=keys, inplace=True)\n",
    "\n",
    "    out['response'] = out['response'].apply(lambda x: [sign(y) for y in x])\n",
    "\n",
    "    if experiment == \"gerrig\":\n",
    "        out['response'] = out['response'].apply(lambda x: [x[1]])\n",
    "    if experiment == \"brewer\":\n",
    "        def reform_id(id):\n",
    "            return id.split(\",\")[0].split(\" Chunks\")[0] + \",\" + id.split(\",\")[1].split(\" Chunks\")[0]\n",
    "        out['id'] = out['id'].apply(reform_id)\n",
    "\n",
    "    return out[['id', 'model', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lehne = {\n",
    "    'Experiment,Normal': [5.565217391, 5, 4.826086957, 5.739130435, 5.52173913, 6.826086957, 7.304347826, 5.434782609, 6.391304348, 7.47826087, 7.043478261, 5.869565217, 6.739130435, 6.956521739, 6.47826087, 5.956521739, 4.652173913, 4.260869565, 5.173913043, 4.086956522, 4.173913043, 4.304347826, 5, 4.043478261, 4.217391304, 4.434782609, 5.347826087, 6.217391304, 5.434782609, 4.782608696, 6.173913043, 5.956521739, 6.47826087, 5, 4.739130435, 5.173913043, 6.304347826, 6.434782609, 5.260869565, 5.304347826, 5.956521739, 4.304347826, 5.260869565, 4.391304348, 4.956521739, 5.695652174, 5.043478261, 5.826086957, 5.043478261, 4.913043478, 5.217391304, 6.217391304, 6.391304348, 6.52173913, 7.217391304, 6.565217391, 5.52173913, 4.347826087, 3.869565217, 7, 7.565217391, 6.52173913, 6.260869565, 6.043478261, 4.913043478]\n",
    "}\n",
    "\n",
    "# Scuffed, but it works\n",
    "delatorre_global_ratings = [3.34, 3.725, 3.705, 3.89, 4.08, 5.02, 4.87, 4.81, 5.84, 5.77, 6.44, 4.685]\n",
    "delatorre_unique_categories = ['Experiment,Journalistic Bad Not Revealed',\n",
    "       'Experiment,Journalistic Bad Revealed',\n",
    "       'Experiment,Journalistic Good Not Revealed',\n",
    "       'Experiment,Journalistic Good Revealed',\n",
    "       'Experiment,Novel Bad Not Revealed',\n",
    "       'Experiment,Novel Bad Revealed',\n",
    "       'Experiment,Novel Good Not Revealed',\n",
    "       'Experiment,Novel Good Revealed']\n",
    "\n",
    "delatorre = {\n",
    "    k: delatorre_global_ratings for k in delatorre_unique_categories\n",
    "}\n",
    "\n",
    "brewer = {\n",
    "    'Experiment A,American Story Birthday' : 3.2,\n",
    "    'Experiment A,American Story Flying' : 3.6,\n",
    "    'Experiment A,American Story Lottery' : 4.5,\n",
    "    'Experiment A,American Story Old Phoebe' : 3.4,\n",
    "    'Experiment A,American Story Ylla' : 5.1,\n",
    "}\n",
    "\n",
    "gerrig = { # Standard suspense / Q2 ratings\n",
    "    \"Experiment A,Pen Not Mentioned\": (3.78 + 3.43) / 2,\n",
    "    \"Experiment A,Pen Mentioned Removed\": (4.38 + 4.06) / 2,\n",
    "    \"Experiment A,Pen Mentioned Not Removed\": 3.47,\n",
    "    \"Experiment B,Unused Comb\": 3.96,\n",
    "    \"Experiment B,Used Comb\": 3.41,\n",
    "    \"Experiment C,Prior Solution Not Mentioned\": (3.76 + 3.34) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned and Removed\": (4.61 + 3.99) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned Not Removed\": 4.14\n",
    "}\n",
    "\n",
    "def normalize_scalar(n, experiment):\n",
    "    likert_midpoint = (LIKERT_BOUNDS[experiment][1] - LIKERT_BOUNDS[experiment][0]) / 2\n",
    "    normalized = (n - likert_midpoint) / likert_midpoint\n",
    "    if normalized > 0:\n",
    "        return 1\n",
    "    elif normalized < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "def normalize_human_ratings(experiment):\n",
    "    exp_dict = globals()[experiment]\n",
    "    for key in exp_dict.keys():\n",
    "        if type(exp_dict[key]) == list:\n",
    "            exp_dict[key] = [normalize_scalar(n, experiment) for n in exp_dict[key]]\n",
    "        else:\n",
    "            exp_dict[key] = [normalize_scalar(exp_dict[key], experiment)]\n",
    "    return exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerrig_human_ratings = normalize_human_ratings(\"gerrig\")\n",
    "brewer_human_ratings = normalize_human_ratings(\"brewer\")\n",
    "lehne_human_ratings = normalize_human_ratings(\"lehne\")\n",
    "delatorre_human_ratings = normalize_human_ratings(\"delatorre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "\n",
    "gerrig_experiment_data = get_experiment_name(\"gerrig\", other_sources)\n",
    "brewer_experiment_data = get_experiment_name(\"brewer\", brewer_sources)\n",
    "lehne_experiment_data = get_experiment_name(\"lehne\", other_sources)\n",
    "delatorre_experiment_data = get_experiment_name(\"delatorre\", other_sources)\n",
    "\n",
    "def get_metrics(experiment_data, human_ratings, experiment_name):\n",
    "    for row in experiment_data.iterrows():\n",
    "        ground_truth = human_ratings[row[1]['id']]\n",
    "        predictions = row[1]['response']\n",
    "\n",
    "        if len(ground_truth) == 1 and len(predictions) > 1:\n",
    "            ground_truth = ground_truth * len(predictions)\n",
    "\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for i in range(len(ground_truth)):\n",
    "            if pd.isna(ground_truth[i]) or pd.isna(predictions[i]):\n",
    "                continue\n",
    "            if ground_truth[i] == 1 and predictions[i] == 1:\n",
    "                tp += 1\n",
    "            elif ground_truth[i] == 0 and predictions[i] == 0:\n",
    "                tn += 1\n",
    "            elif ground_truth[i] == 0 and predictions[i] == 1:\n",
    "                fp += 1\n",
    "            elif ground_truth[i] == 1 and predictions[i] == 0:\n",
    "                fn += 1\n",
    "\n",
    "        if tp + tn + fp + fn != 0:\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            num_valid = len([x for x in predictions if not pd.isna(x)])\n",
    "            num_invalid = len([x for x in predictions if pd.isna(x)])\n",
    "        else:\n",
    "            accuracy = float('nan')\n",
    "            precision = float('nan')\n",
    "            recall = float('nan')\n",
    "            f1 = float('nan')\n",
    "            num_valid = 0\n",
    "            num_invalid = len(predictions)\n",
    "\n",
    "        # print(f\"Valid: {len([x for x in predictions if not pd.isna(x)])}, Invalid: {len([x for x in predictions if pd.isna(x)])}\")\n",
    "        experiment_data.at[row[0], 'accuracy'] = accuracy\n",
    "        experiment_data.at[row[0], 'precision'] = precision\n",
    "        experiment_data.at[row[0], 'recall'] = recall\n",
    "        experiment_data.at[row[0], 'f1'] = f1\n",
    "        experiment_data.at[row[0], 'num_valid'] = num_valid\n",
    "        experiment_data.at[row[0], 'num_invalid'] = num_invalid\n",
    "\n",
    "    metrics_summary = experiment_data.groupby(['model']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'num_valid': ['sum'],\n",
    "        'num_invalid': ['sum']\n",
    "    })\n",
    "\n",
    "    # Add a summary row with means\n",
    "    metrics_summary.loc['Average'] = metrics_summary.mean()\n",
    "\n",
    "    return metrics_summary\n",
    "\n",
    "    # # predictions = experiment_data.iloc[:, -len(value_cols):]\n",
    "    # print(predictions, human_ratings)\n",
    "    # conf_matrices = predictions.apply(lambda row: confusion_matrix(human_ratings, row, labels=[0, 1]), axis=1)\n",
    "\n",
    "\n",
    "brewer_metrics = get_metrics(brewer_experiment_data, brewer_human_ratings, \"brewer\")\n",
    "gerrig_metrics = get_metrics(gerrig_experiment_data, gerrig_human_ratings, \"gerrig\")\n",
    "delatorre_metrics = get_metrics(delatorre_experiment_data, delatorre_human_ratings, \"delatorre\")\n",
    "lehne_metrics = get_metrics(lehne_experiment_data, lehne_human_ratings, \"lehne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name(model : str):\n",
    "    excluded = [\"Average\", \"Consensus\"]\n",
    "    if model in excluded:\n",
    "        return model\n",
    "    model_name = model.split(\"_\")[-1]\n",
    "    model_name = model_name.split(\"-Instruct\")[0]\n",
    "    model_name = model_name.split(\"-chat\")[0]\n",
    "    model_name = model_name.split(\"-it\")[0]\n",
    "    model_name = model_name.replace(\"-\", \" \")\n",
    "    model_name = model_name[0].upper() + model_name[1:]\n",
    "    return model_name\n",
    "\n",
    "def prettify_table(table):\n",
    "    table = table.copy()\n",
    "    numerics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    def format_mean_std(x):\n",
    "        if pd.isna(x['std']):\n",
    "            return f\"{x['mean']:.2f} ± 0.0\"\n",
    "        return f\"{x['mean']:.2f} ± {x['std']:.2f}\"\n",
    "    for metric in numerics:\n",
    "        table[metric, 'mean'] = table[metric, 'mean'].apply(lambda x: round(x, 2))\n",
    "        table[metric, 'std'] = table[metric, 'std'].apply(lambda x: round(x, 2))\n",
    "        collapsed = table[metric].apply(format_mean_std, axis=1)\n",
    "        table[metric] = collapsed\n",
    "        table.drop(columns=[(metric, 'std')], inplace=True)\n",
    "    table.columns = table.columns.droplevel(1)\n",
    "    table = table.reset_index()\n",
    "    table['model'] = table['model'].apply(format_model_name)\n",
    "\n",
    "    table.set_index('model', inplace=True)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_gerrig = prettify_table(gerrig_metrics)\n",
    "pretty_brewer = prettify_table(brewer_metrics)\n",
    "pretty_delatorre = prettify_table(delatorre_metrics)\n",
    "pretty_lehne = prettify_table(lehne_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty_gerrig\n",
    "# pretty_brewer\n",
    "# pretty_delatorre\n",
    "pretty_lehne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_single_metric(tables : list[pd.DataFrame], metric : str, labels : list[str] = None):\n",
    "    out = pd.concat(tables, axis=1)\n",
    "    out = out[[metric]]\n",
    "    if labels:\n",
    "        out.columns = labels\n",
    "    out = out.reset_index()\n",
    "    for col in out.columns:\n",
    "        col = col.upper()\n",
    "\n",
    "    return out.reset_index()\n",
    "\n",
    "def as_latex(table):\n",
    "    table = table.drop(columns=['index'])\n",
    "    table = table.rename(columns={'model': 'Model'})\n",
    "    return table.to_latex(index=False, escape=False)\n",
    "\n",
    "joined = join_with_single_metric([pretty_gerrig, pretty_brewer, pretty_delatorre, pretty_lehne], 'f1', labels=['Gerrig', 'Brewer', 'Delatorre', 'Lehne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(as_latex(joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delatorre_experiment_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
