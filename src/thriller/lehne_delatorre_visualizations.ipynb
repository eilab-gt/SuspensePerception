{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from itertools import zip_longest\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Lehne\"\n",
    "# target = \"Delatorre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zipped_average(lists):\n",
    "    zipped_lists = zip_longest(*lists, fillvalue=None)\n",
    "    averages = [\n",
    "        sum(filter(None.__ne__, group)) / max(1, len(list(filter(None.__ne__, group))))\n",
    "        for group in zipped_lists\n",
    "    ]\n",
    "    return averages\n",
    "\n",
    "def get_llm_ratings(llm_rating_sources: list[str]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the average llm rating for a given source.\n",
    "    Args:\n",
    "        llm_rating_sources: list of llm rating sources. This is the dirname to start checking for csv files from.\n",
    "                            Subdirectories will be checked starting from this directory.\n",
    "    Return:\n",
    "        List of all llm ratings for the given source.\n",
    "        If multiple sources are given return the average rating across all sources.\n",
    "    \"\"\"\n",
    "    all_source_ratings = []\n",
    "\n",
    "    for source in llm_rating_sources:\n",
    "        source = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "        source_ratings = []\n",
    "        \n",
    "        # Traverse through the directory to find CSV files\n",
    "        for root, _, files in os.walk(source):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # Read the CSV file\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Assuming the response column contains the dictionary of ratings as string\n",
    "                    if 'response' in df.columns:\n",
    "                        model_ratings = []\n",
    "                        for response in df['response']:\n",
    "                            response_dict = ast.literal_eval(response)\n",
    "                            ratings = response_dict.values()\n",
    "                            model_ratings.append(ratings)\n",
    "                        source_ratings = get_zipped_average(model_ratings)\n",
    "        \n",
    "        if source_ratings:\n",
    "            all_source_ratings.append(source_ratings)\n",
    "\n",
    "    llm_ratings = get_zipped_average(all_source_ratings)\n",
    "    return llm_ratings\n",
    "\n",
    "def get_per_model_ratings(llm_rating_sources: list[str]) -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Get the llm ratings from all sources.\n",
    "    Args:\n",
    "        llm_rating_sources: list of llm rating sources. This is the dirname to start checking for csv files from.\n",
    "                            Subdirectories will be checked starting from this directory.\n",
    "    Return:\n",
    "        Dict of all llm ratings from all sources.\n",
    "        Keyed by model name to list of scores.\n",
    "    \"\"\"\n",
    "    all_ratings = {}\n",
    "\n",
    "    experiments = [\"e1\", \"e2\", \"e3\"]\n",
    "    model_names = ['deepseek-ai_DeepSeek-V3', 'google_gemma-2-27b-it', 'google_gemma-2-9b-it', 'meta-llama_Llama-2-7b-chat-hf', 'meta-llama_Llama-3-70b-chat-hf', 'meta-llama_Llama-3-8b-chat-hf', 'microsoft_WizardLM-2-8x22B', 'mistralai_Mistral-7B-Instruct-v0.3', 'mistralai_Mixtral-8x7B-Instruct-v0.1', 'Qwen_Qwen2-72B-Instruct']\n",
    "    shortened_model_names = [\"DS-V3\", \"G-27B\", \"G-9B\", 'L2-7B', 'L3-70B', 'L3-8B', 'W-22B', 'M-7B', 'Mx-7B', 'Q-72B']\n",
    "\n",
    "    for source in llm_rating_sources:\n",
    "        source = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "\n",
    "        for experiment in experiments:\n",
    "            for i, model in enumerate(model_names):\n",
    "                csv_path = os.path.join(source, experiment, model)\n",
    "                csv_path = os.path.join(csv_path, os.listdir(csv_path)[0], \"results.csv\")\n",
    "\n",
    "                # Read the CSV file\n",
    "                ratings = []\n",
    "                df = pd.read_csv(csv_path)\n",
    "                if 'response' in df.columns:\n",
    "                    for response in df['response']:\n",
    "                        response_dict = ast.literal_eval(response)\n",
    "                        ratings = list(response_dict.values())\n",
    "\n",
    "                shortened_model_name = shortened_model_names[i]\n",
    "                if shortened_model_name not in all_ratings:\n",
    "                    all_ratings[shortened_model_name] = []\n",
    "                all_ratings[shortened_model_name].append(ratings)\n",
    "\n",
    "    for shortened_model_name, ratings in all_ratings.items():\n",
    "        all_ratings[shortened_model_name] = get_zipped_average(ratings)\n",
    "\n",
    "    return all_ratings\n",
    "\n",
    "def get_adversarial_ratings(llm_adversarial_sources: list[str]) -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Get the llm ratings from all adversarial sources.\n",
    "    Args:\n",
    "        llm_adversarial_sources: list of llm rating sources. This is the dirname to start checking for csv files from.\n",
    "                            Subdirectories will be checked starting from this directory.\n",
    "    Return:\n",
    "        Dict of all llm ratings from all sources.\n",
    "        Keyed by adversarial attack name to list of scores.\n",
    "    \"\"\"\n",
    "    all_ratings = {}\n",
    "\n",
    "    attacks = [\"antonym_replacement\", \"caesar_cipher\", \"change_character_names\", \"context_removal\", \"control\", \"distraction_insertion\", \"introduce_typos\", \"shuffle_sentences\", \"swap_words\", \"synonym_replacement\", \"word_swap_embedding\", \"word_swap_homoglyph\"]\n",
    "    experiments = [\"e0\", \"e1\", \"e2\"]\n",
    "    model_names = ['deepseek-ai_DeepSeek-V3', 'google_gemma-2-27b-it', 'google_gemma-2-9b-it', 'meta-llama_Llama-2-7b-chat-hf', 'meta-llama_Llama-3-70b-chat-hf', 'meta-llama_Llama-3-8b-chat-hf', 'microsoft_WizardLM-2-8x22B', 'mistralai_Mistral-7B-Instruct-v0.3', 'mistralai_Mixtral-8x7B-Instruct-v0.1', 'Qwen_Qwen2-72B-Instruct']\n",
    "\n",
    "    for source in llm_adversarial_sources:\n",
    "        source = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}/../outputs/{source}\"\n",
    "\n",
    "        for attack in attacks:\n",
    "\n",
    "            if attack not in all_ratings:\n",
    "                all_ratings[attack] = []\n",
    "\n",
    "            for experiment in experiments:\n",
    "                for i, model in enumerate(model_names):\n",
    "                    csv_path = os.path.join(source, attack, experiment, model)\n",
    "                    csv_path = os.path.join(csv_path, os.listdir(csv_path)[0], \"results.csv\")\n",
    "\n",
    "                    # Read the CSV file\n",
    "                    ratings = []\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    if 'response' in df.columns:\n",
    "                        for response in df['response']:\n",
    "                            response_dict = ast.literal_eval(response)\n",
    "                            ratings = list(response_dict.values())\n",
    "\n",
    "                    all_ratings[attack].append(ratings)\n",
    "\n",
    "    for attack, ratings in all_ratings.items():\n",
    "        all_ratings[attack] = get_zipped_average(ratings)\n",
    "\n",
    "    return all_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_rating_sources = None\n",
    "llm_adversarial_sources = None\n",
    "if target == \"Lehne\":\n",
    "    llm_rating_sources = [\"lehne_experiment/final/\"]\n",
    "    llm_adversarial_sources = [\"lehne_experiment/adversarial/\"]\n",
    "elif target == \"Delatorre\":\n",
    "    llm_rating_sources = [\"delatorre_experiment/final/\"]\n",
    "    llm_adversarial_sources = [\"delatorre_experiment/adversarial/\"]\n",
    "\n",
    "llm_ratings = get_llm_ratings(llm_rating_sources)\n",
    "llm_ratings_by_model = get_per_model_ratings(llm_rating_sources)\n",
    "try:\n",
    "    llm_ratings_adversarial = get_adversarial_ratings(llm_adversarial_sources)\n",
    "except:\n",
    "    llm_ratings_adversarial = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_ratings = None\n",
    "if target == \"Lehne\":\n",
    "    human_ratings = [5.565217391, 5, 4.826086957, 5.739130435, 5.52173913, 6.826086957, 7.304347826, 5.434782609, 6.391304348, 7.47826087, 7.043478261, 5.869565217, 6.739130435, 6.956521739, 6.47826087, 5.956521739, 4.652173913, 4.260869565, 5.173913043, 4.086956522, 4.173913043, 4.304347826, 5, 4.043478261, 4.217391304, 4.434782609, 5.347826087, 6.217391304, 5.434782609, 4.782608696, 6.173913043, 5.956521739, 6.47826087, 5, 4.739130435, 5.173913043, 6.304347826, 6.434782609, 5.260869565, 5.304347826, 5.956521739, 4.304347826, 5.260869565, 4.391304348, 4.956521739, 5.695652174, 5.043478261, 5.826086957, 5.043478261, 4.913043478, 5.217391304, 6.217391304, 6.391304348, 6.52173913, 7.217391304, 6.565217391, 5.52173913, 4.347826087, 3.869565217, 7, 7.565217391, 6.52173913, 6.260869565, 6.043478261, 4.913043478]\n",
    "elif target == \"Delatorre\":\n",
    "    human_ratings = [3.34, 3.725, 3.705, 3.89, 4.08, 5.02, 4.87, 4.81, 5.84, 5.77, 6.44, 4.685]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = None\n",
    "fontsize = None\n",
    "if target == \"Lehne\":\n",
    "    figsize = (40, 11)\n",
    "    fontsize = 23\n",
    "elif target == \"Delatorre\":\n",
    "    figsize = (16, 11)\n",
    "    fontsize = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage number (0-indexed) that the story changes from rising action to falling action, and vice versa\n",
    "\n",
    "inflection_points = []\n",
    "if target == \"Lehne\":\n",
    "    inflection_points = [10, 11, 12, 14, 15, 21, 22, 23, 32, 33, 35, 36, 48, 49, 50, 51, 55, 56, 58, 59, 63, 64]\n",
    "elif target == \"Delatorre\":\n",
    "    inflection_points = [9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(llm_ratings_by_model.keys()) + [\"H\"]\n",
    "llm_ratings = np.array(list(llm_ratings_by_model.values()) + [human_ratings])\n",
    "\n",
    "# Compute agreement matrix (1 for agreement, 0 for disagreement)\n",
    "max_diff = np.max(np.abs(llm_ratings - human_ratings))\n",
    "agreement_matrix = 1 - (np.abs(llm_ratings - human_ratings) / max_diff)  # Normalize between 0 and 1\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=figsize)\n",
    "ax = sns.heatmap(agreement_matrix, vmin=0, vmax=1, annot=llm_ratings, cmap=\"viridis\", cbar=True, linewidths=0, xticklabels=range(1, llm_ratings.shape[1] + 1), yticklabels=model_names)\n",
    "\n",
    "plt.title(f\"{target} Average Ratings by Model (1 = Agreement, 0 = Disagreement)\", fontsize=fontsize)\n",
    "plt.xlabel(\"Passage\", fontsize=fontsize)\n",
    "plt.ylabel(\"Model\", fontsize=fontsize)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_func(value):\n",
    "    return f\"+{value:.1f}\" if value >= 0 else f\"{value:.1f}\"\n",
    "\n",
    "model_names = list(llm_ratings_by_model.keys()) + [\"H\"]\n",
    "llm_ratings = np.array(list(llm_ratings_by_model.values()) + [human_ratings])\n",
    "\n",
    "llm_ratings_change = np.diff(llm_ratings)\n",
    "human_ratings_change = np.diff(human_ratings)\n",
    "\n",
    "annot_fmt = np.vectorize(fmt_func)(llm_ratings_change)\n",
    "\n",
    "# Compute agreement matrix (1 for agreement, 0 for disagreement)\n",
    "max_diff = np.max(np.abs(llm_ratings_change - human_ratings_change))\n",
    "agreement_matrix = 1 - (np.abs(llm_ratings_change - human_ratings_change) / max_diff)  # Normalize between 0 and 1\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=figsize)\n",
    "ax = sns.heatmap(agreement_matrix, vmin=0, vmax=1, annot=annot_fmt, cmap=\"viridis\", cbar=True, linewidths=0.0, xticklabels=range(2, llm_ratings_change.shape[1] + 2), yticklabels=model_names, fmt=\"\")\n",
    "\n",
    "plt.title(f\"{target} Average Ratings Change by Model (1 = Agreement, 0 = Disagreement)\", fontsize=fontsize)\n",
    "plt.xlabel(\"Passage\", fontsize=fontsize)\n",
    "plt.ylabel(\"Model\", fontsize=fontsize)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(llm_ratings_by_model.keys()) + [\"H\"]\n",
    "llm_ratings = np.array(list(llm_ratings_by_model.values()) + [human_ratings])\n",
    "\n",
    "llm_ratings_change = np.diff(llm_ratings)\n",
    "human_ratings_change = np.diff(human_ratings)\n",
    "\n",
    "llm_ratings_change_direction = np.where(llm_ratings_change > 0, 1, np.where(llm_ratings_change < 0, -1, llm_ratings_change))\n",
    "human_ratings_change_direction = np.where(human_ratings_change > 0, 1, np.where(human_ratings_change < 0, -1, human_ratings_change))\n",
    "\n",
    "# Compute agreement matrix (1 for agreement, 0 for disagreement)\n",
    "max_diff = np.max(np.abs(llm_ratings_change_direction - human_ratings_change_direction))\n",
    "agreement_matrix = 1 - (np.abs(llm_ratings_change_direction - human_ratings_change_direction) / max_diff)  # Normalize between 0 and 1\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=figsize)\n",
    "ax = sns.heatmap(agreement_matrix, vmin=0, vmax=1, annot=llm_ratings_change_direction, cmap=\"viridis\", cbar=True, linewidths=0.0, xticklabels=range(2, llm_ratings_change_direction.shape[1] + 2), yticklabels=model_names)\n",
    "\n",
    "plt.title(f\"{target} Average Ratings Change Direction by Model (1 = Agreement, 0 = Disagreement)\", fontsize=fontsize)\n",
    "plt.xlabel(\"Passage\", fontsize=fontsize)\n",
    "plt.ylabel(\"Model\", fontsize=fontsize)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_ratings_adversarial:\n",
    "    control_ratings = llm_ratings_adversarial[\"control\"]\n",
    "    llm_ratings_adversarial.pop(\"control\")\n",
    "\n",
    "    attack_names = list(llm_ratings_adversarial.keys()) + [\"Control\"]\n",
    "    llm_ratings = np.array(list(llm_ratings_adversarial.values()) + [control_ratings])\n",
    "\n",
    "    # Compute agreement matrix (1 for agreement, 0 for disagreement)\n",
    "    max_diff = np.max(np.abs(llm_ratings - control_ratings))\n",
    "    agreement_matrix = 1 - (np.abs(llm_ratings - control_ratings) / max_diff)  # Normalize between 0 and 1\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(agreement_matrix, vmin=0, vmax=1, annot=llm_ratings, cmap=\"viridis\", cbar=True, linewidths=0, xticklabels=range(1, llm_ratings.shape[1] + 1), yticklabels=attack_names)\n",
    "\n",
    "    plt.title(f\"{target} Average Ratings by Attack (1 = Agreement, 0 = Disagreement)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"Passage\", fontsize=fontsize)\n",
    "    plt.ylabel(\"Attack\", fontsize=fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(llm_ratings_by_model.keys()) + [\"H\"]\n",
    "llm_ratings = np.array(list(llm_ratings_by_model.values()) + [human_ratings])\n",
    "\n",
    "# Compute agreement matrix (1 for agreement, 0 for disagreement)\n",
    "max_diff = np.max(np.abs(llm_ratings - human_ratings))\n",
    "agreement_matrix = 1 - (np.abs(llm_ratings - human_ratings) / max_diff)  # Normalize between 0 and 1\n",
    "\n",
    "# Get only inflection point columns\n",
    "llm_ratings_inflection = llm_ratings[:, inflection_points]\n",
    "agreement_matrix_inflection = agreement_matrix[:, inflection_points]\n",
    "xticklabels = np.array(range(1, llm_ratings.shape[1] + 1))[inflection_points]\n",
    "\n",
    "print(np.mean(agreement_matrix))\n",
    "print(np.mean(agreement_matrix_inflection))\n",
    "foobar = np.delete(agreement_matrix, inflection_points)\n",
    "print(np.mean(foobar))\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=figsize)\n",
    "ax = sns.heatmap(agreement_matrix_inflection, vmin=0, vmax=1, annot=llm_ratings_inflection, cmap=\"viridis\", cbar=True, linewidths=0, xticklabels=xticklabels, yticklabels=model_names)\n",
    "\n",
    "plt.title(f\"{target} Average Ratings by Model, Inflection Points Only (1 = Agreement, 0 = Disagreement)\", fontsize=fontsize)\n",
    "plt.xlabel(\"Passage\", fontsize=fontsize)\n",
    "plt.ylabel(\"Model\", fontsize=fontsize)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
