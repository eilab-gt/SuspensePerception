experiment:
  experiment_series: "gerrig"
  use_alternative: false
  output_dir: "./outputs/gerrig_experiment/final/paper/exp3"
model:
  api_type: "together"
  name: [
    #"meta-llama/Llama-3-8b-chat-hf",
    #"meta-llama/Llama-3-70b-chat-hf",
    #"meta-llama/Llama-2-7b-chat-hf",
    "meta-llama/Llama-2-13b-chat-hf",
    #"google/gemma-2-9b-it",
    #"google/gemma-2-27b-it",
    #"mistralai/Mistral-7B-Instruct-v0.3",
   #"mistralai/Mixtral-8x7B-Instruct-v0.1",
    #"Qwen/Qwen2-72B-Instruct",
    "deepseek-ai/DeepSeek-V3",
    "microsoft/WizardLM-2-8x22B", 
  ]
  max_tokens: 20000
  temperature: 0.0
  top_p: 0.9
  repetition_penalty: 1.0
  stop: ["<|endoftext|>"]
  stream: true
parse_model:
  api_type: "together"
  name: "meta-llama/Llama-3-8b-chat-hf"
  prompt: >
    The text below is the output of a question and answer prompt to a language model. There are only two questions (Q1, Q2). For each question the answer is one of the following choices:

    Q1:
    1. Not very likely
    2. Somewhat likely
    3. Slightly likely
    4. Neutral or Uncertain
    5. Moderately likely
    6. Very likely
    7. Extremely likely

    Q2:
    1. Not very suspenseful
    2. Somewhat suspenseful
    3. Slightly suspenseful
    4. Neutral or Uncertain
    5. Moderately suspenseful
    6. Very suspenseful
    7. Extremely suspenseful
    
    Extract the numerical response from the answer. When generating you should structure your response to this prompt by using the format: ```Q1: answer\nQ2: answer``` and exclude all other questions mentioned.
  max_tokens: 100
  temperature: 0.0 # Parser should be deterministic
  top_p: 0.9
  repetition_penalty: 1.0
  stop: ["<|endoftext|>"]
  stream: true
