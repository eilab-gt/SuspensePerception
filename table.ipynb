{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ast\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = glob.glob(\"outputs/**/*.csv\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "brewer_sources = [\"paper/exp1\", \"e3\"]\n",
    "other_sources = [\"e1\", \"e2\", \"e3\"]\n",
    "experiments = [\"brewer\", \"delatorre\", \"gerrig\", \"lehne\"]\n",
    "ROOT = os.path.dirname(\"outputs/\")\n",
    "\n",
    "LIKERT_BOUNDS = {\n",
    "    \"delatorre\": [1, 9],\n",
    "    \"brewer\": [1, 7],\n",
    "    \"gerrig\": [1, 7],\n",
    "    \"lehne\": [1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "def get_experiment_name(experiment, runs):\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    \n",
    "    data_blocks = glob.glob(f\"outputs/{experiment}_experiment/final/**/results.csv\", recursive=True)\n",
    "\n",
    "    likert_midpoint = (LIKERT_BOUNDS[experiment][1] - LIKERT_BOUNDS[experiment][0]) / 2\n",
    "\n",
    "    sign = lambda x: 1 if x > likert_midpoint else (0 if x < likert_midpoint else float('nan'))\n",
    "\n",
    "    keys = None\n",
    "    for block_path in data_blocks:\n",
    "\n",
    "        mat = re.search(r\"outputs\\/(.+)_experiment\\/final\\/(.+)\\/(.+)\\/(.+)\\/results.csv\", block_path)\n",
    "        model = mat.group(3)\n",
    "        if mat.group(2) not in runs:\n",
    "            continue\n",
    "        block = pd.read_csv(block_path)\n",
    "        block['model'] = model\n",
    "        block['run'] = mat.group(2)\n",
    "\n",
    "        if experiment == \"brewer\":\n",
    "            block = block[block['version'].str.contains(\"chunks\", case=False)]\n",
    "\n",
    "        try:\n",
    "            responses = block['response'].apply(ast.literal_eval)\n",
    "        except:\n",
    "            continue\n",
    "        responses = responses.apply(pd.Series)\n",
    "        if experiment == \"brewer\":\n",
    "            brewer_responses = pd.DataFrame(columns=[\"0\", \"3\", \"6\", \"9\", \"12\"])\t\n",
    "            responses = responses[[col for col in [\"0\", \"3\", \"6\", \"9\", \"12\"] if col in responses.columns]]\n",
    "            responses = pd.concat([brewer_responses, responses], axis=0)\n",
    "        block = pd.concat([block, responses], axis=1).drop(columns=['response'])\n",
    "        out = pd.concat([out, block], axis=0)\n",
    "\n",
    "        keys = responses.columns\n",
    "\n",
    "    out['id'] = out['experiment_name'] + \",\" + out['version']\n",
    "\n",
    "    for key in keys:\n",
    "        out[key] = out[key].astype(float)\n",
    "\n",
    "    out = out.drop(columns=['experiment_name', 'version'])\n",
    "\n",
    "    out = out.groupby(['id', 'model']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "    out['response'] = out[keys].apply(lambda x: x.to_list(), axis=1)\n",
    "    out.drop(columns=keys, inplace=True)\n",
    "\n",
    "    out['response'] = out['response'].apply(lambda x: [sign(y) for y in x])\n",
    "\n",
    "    if experiment == \"gerrig\":\n",
    "        out['response'] = out['response'].apply(lambda x: [x[1]])\n",
    "    if experiment == \"brewer\":\n",
    "        def reform_id(id):\n",
    "            return id.split(\",\")[0].split(\" Chunks\")[0] + \",\" + id.split(\",\")[1].split(\" Chunks\")[0]\n",
    "        out['id'] = out['id'].apply(reform_id)\n",
    "\n",
    "    return out[['id', 'model', 'response']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "lehne = {\n",
    "    'Experiment,Normal': [5.565217391, 5, 4.826086957, 5.739130435, 5.52173913, 6.826086957, 7.304347826, 5.434782609, 6.391304348, 7.47826087, 7.043478261, 5.869565217, 6.739130435, 6.956521739, 6.47826087, 5.956521739, 4.652173913, 4.260869565, 5.173913043, 4.086956522, 4.173913043, 4.304347826, 5, 4.043478261, 4.217391304, 4.434782609, 5.347826087, 6.217391304, 5.434782609, 4.782608696, 6.173913043, 5.956521739, 6.47826087, 5, 4.739130435, 5.173913043, 6.304347826, 6.434782609, 5.260869565, 5.304347826, 5.956521739, 4.304347826, 5.260869565, 4.391304348, 4.956521739, 5.695652174, 5.043478261, 5.826086957, 5.043478261, 4.913043478, 5.217391304, 6.217391304, 6.391304348, 6.52173913, 7.217391304, 6.565217391, 5.52173913, 4.347826087, 3.869565217, 7, 7.565217391, 6.52173913, 6.260869565, 6.043478261, 4.913043478]\n",
    "}\n",
    "\n",
    "# Scuffed, but it works\n",
    "delatorre_global_ratings = [3.34, 3.725, 3.705, 3.89, 4.08, 5.02, 4.87, 4.81, 5.84, 5.77, 6.44, 4.685]\n",
    "delatorre_unique_categories = ['Experiment,Journalistic Bad Not Revealed',\n",
    "       'Experiment,Journalistic Bad Revealed',\n",
    "       'Experiment,Journalistic Good Not Revealed',\n",
    "       'Experiment,Journalistic Good Revealed',\n",
    "       'Experiment,Novel Bad Not Revealed',\n",
    "       'Experiment,Novel Bad Revealed',\n",
    "       'Experiment,Novel Good Not Revealed',\n",
    "       'Experiment,Novel Good Revealed']\n",
    "\n",
    "delatorre = {\n",
    "    k: delatorre_global_ratings for k in delatorre_unique_categories\n",
    "}\n",
    "\n",
    "brewer = {\n",
    "    'Experiment A,American Story Birthday' : 3.2,\n",
    "    'Experiment A,American Story Flying' : 3.6,\n",
    "    'Experiment A,American Story Lottery' : 4.5,\n",
    "    'Experiment A,American Story Old Phoebe' : 3.4,\n",
    "    'Experiment A,American Story Ylla' : 5.1,\n",
    "}\n",
    "\n",
    "gerrig = { # Standard suspense / Q2 ratings\n",
    "    \"Experiment A,Pen Not Mentioned\": (3.78 + 3.43) / 2,\n",
    "    \"Experiment A,Pen Mentioned Removed\": (4.38 + 4.06) / 2,\n",
    "    \"Experiment A,Pen Mentioned Not Removed\": 3.47,\n",
    "    \"Experiment B,Unused Comb\": 3.96,\n",
    "    \"Experiment B,Used Comb\": 3.41,\n",
    "    \"Experiment C,Prior Solution Not Mentioned\": (3.76 + 3.34) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned and Removed\": (4.61 + 3.99) / 2,\n",
    "    \"Experiment C,Prior Solution Mentioned Not Removed\": 4.14\n",
    "}\n",
    "\n",
    "def normalize_scalar(n, experiment):\n",
    "    likert_midpoint = (LIKERT_BOUNDS[experiment][1] - LIKERT_BOUNDS[experiment][0]) / 2\n",
    "    normalized = (n - likert_midpoint) / likert_midpoint\n",
    "    if normalized > 0:\n",
    "        return 1\n",
    "    elif normalized < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "def normalize_human_ratings(experiment):\n",
    "    exp_dict = globals()[experiment]\n",
    "    for key in exp_dict.keys():\n",
    "        if type(exp_dict[key]) == list:\n",
    "            exp_dict[key] = [normalize_scalar(n, experiment) for n in exp_dict[key]]\n",
    "        else:\n",
    "            exp_dict[key] = [normalize_scalar(exp_dict[key], experiment)]\n",
    "    return exp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerrig_human_ratings = normalize_human_ratings(\"gerrig\")\n",
    "brewer_human_ratings = normalize_human_ratings(\"brewer\")\n",
    "lehne_human_ratings = normalize_human_ratings(\"lehne\")\n",
    "delatorre_human_ratings = normalize_human_ratings(\"delatorre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "\n",
    "gerrig_experiment_data = get_experiment_name(\"gerrig\", other_sources)\n",
    "brewer_experiment_data = get_experiment_name(\"brewer\", brewer_sources)\n",
    "lehne_experiment_data = get_experiment_name(\"lehne\", other_sources)\n",
    "delatorre_experiment_data = get_experiment_name(\"delatorre\", other_sources)\n",
    "\n",
    "def get_metrics(experiment_data, human_ratings, experiment_name):\n",
    "    \n",
    "    for row in experiment_data.iterrows():\n",
    "        ground_truth = human_ratings[row[1]['id']]\n",
    "        predictions = row[1]['response']\n",
    "\n",
    "        if len(ground_truth) == 1 and len(predictions) > 1:\n",
    "            ground_truth = ground_truth * len(predictions)\n",
    "\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for i in range(len(ground_truth)):\n",
    "            if pd.isna(ground_truth[i]) or pd.isna(predictions[i]):\n",
    "                continue\n",
    "            if ground_truth[i] == 1 and predictions[i] == 1:\n",
    "                tp += 1\n",
    "            elif ground_truth[i] == 0 and predictions[i] == 0:\n",
    "                tn += 1\n",
    "            elif ground_truth[i] == 0 and predictions[i] == 1:\n",
    "                fp += 1\n",
    "            elif ground_truth[i] == 1 and predictions[i] == 0:\n",
    "                fn += 1\n",
    "\n",
    "        if tp + tn + fp + fn != 0:\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            num_valid = len([x for x in predictions if not pd.isna(x)])\n",
    "            num_invalid = len([x for x in predictions if pd.isna(x)])\n",
    "        else:\n",
    "            accuracy = float('nan')\n",
    "            precision = float('nan')\n",
    "            recall = float('nan')\n",
    "            f1 = float('nan')\n",
    "            num_valid = 0\n",
    "            num_invalid = len(predictions)\n",
    "    \n",
    "\n",
    "        # print(f\"Valid: {len([x for x in predictions if not pd.isna(x)])}, Invalid: {len([x for x in predictions if pd.isna(x)])}\")\n",
    "        experiment_data.at[row[0], 'accuracy'] = accuracy\n",
    "        experiment_data.at[row[0], 'precision'] = precision\n",
    "        experiment_data.at[row[0], 'recall'] = recall\n",
    "        experiment_data.at[row[0], 'f1'] = f1\n",
    "        experiment_data.at[row[0], 'num_valid'] = num_valid\n",
    "        experiment_data.at[row[0], 'num_invalid'] = num_invalid\n",
    "\n",
    "    metrics_summary = experiment_data.groupby(['model']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'num_valid': ['sum'],\n",
    "        'num_invalid': ['sum']\n",
    "    })\n",
    "\n",
    "    # Add a summary row with means\n",
    "    metrics_summary.loc['Average'] = metrics_summary.mean()\n",
    "\n",
    "    return metrics_summary\n",
    "\n",
    "    # # predictions = experiment_data.iloc[:, -len(value_cols):]\n",
    "    # print(predictions, human_ratings)\n",
    "    # conf_matrices = predictions.apply(lambda row: confusion_matrix(human_ratings, row, labels=[0, 1]), axis=1)\n",
    "\n",
    "\n",
    "brewer_metrics = get_metrics(brewer_experiment_data, brewer_human_ratings, \"brewer\")\n",
    "# gerrig_metrics = get_metrics(gerrig_experiment_data, gerrig_human_ratings, \"gerrig\")\n",
    "# delatorre_metrics = get_metrics(delatorre_experiment_data, delatorre_human_ratings, \"delatorre\")\n",
    "# lehne_metrics = get_metrics(lehne_experiment_data, lehne_human_ratings, \"lehne\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_name(model : str):\n",
    "    excluded = [\"Average\", \"Consensus\"]\n",
    "    if model in excluded:\n",
    "        return model\n",
    "    model_name = model.split(\"_\")[-1]\n",
    "    model_name = model_name.split(\"-Instruct\")[0]\n",
    "    model_name = model_name.split(\"-chat\")[0]\n",
    "    model_name = model_name.split(\"-it\")[0]\n",
    "    model_name = model_name.replace(\"-\", \" \")\n",
    "    model_name = model_name[0].upper() + model_name[1:]\n",
    "    return model_name\n",
    "\n",
    "def prettify_table(table):\n",
    "    table = table.copy()\n",
    "    numerics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    def format_mean_std(x):\n",
    "        if pd.isna(x['std']):\n",
    "            return f\"{x['mean']:.2f} ± 0.0\"\n",
    "        return f\"{x['mean']:.2f} ± {x['std']:.2f}\"\n",
    "    for metric in numerics:\n",
    "        table[metric, 'mean'] = table[metric, 'mean'].apply(lambda x: round(x, 2))\n",
    "        table[metric, 'std'] = table[metric, 'std'].apply(lambda x: round(x, 2))\n",
    "        collapsed = table[metric].apply(format_mean_std, axis=1)\n",
    "        table[metric] = collapsed\n",
    "        table.drop(columns=[(metric, 'std')], inplace=True)\n",
    "    table.columns = table.columns.droplevel(1)\n",
    "    table = table.reset_index()\n",
    "    table['model'] = table['model'].apply(format_model_name)\n",
    "\n",
    "    table.set_index('model', inplace=True)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_gerrig = prettify_table(gerrig_metrics)\n",
    "pretty_brewer = prettify_table(brewer_metrics)\n",
    "pretty_delatorre = prettify_table(delatorre_metrics)\n",
    "pretty_lehne = prettify_table(lehne_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>num_valid</th>\n",
       "      <th>num_invalid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Qwen2 72B</th>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepSeek V3</th>\n",
       "      <td>0.93 ± 0.15</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.93 ± 0.15</td>\n",
       "      <td>0.96 ± 0.09</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 27b</th>\n",
       "      <td>0.80 ± 0.27</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.80 ± 0.27</td>\n",
       "      <td>0.87 ± 0.18</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemma 2 9b</th>\n",
       "      <td>0.80 ± 0.33</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.80 ± 0.33</td>\n",
       "      <td>0.85 ± 0.26</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 2 7b</th>\n",
       "      <td>0.19 ± 0.38</td>\n",
       "      <td>0.25 ± 0.50</td>\n",
       "      <td>0.19 ± 0.38</td>\n",
       "      <td>0.21 ± 0.43</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3 70b</th>\n",
       "      <td>0.72 ± 0.31</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.72 ± 0.31</td>\n",
       "      <td>0.80 ± 0.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama 3 8b</th>\n",
       "      <td>0.92 ± 0.18</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.92 ± 0.18</td>\n",
       "      <td>0.95 ± 0.11</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WizardLM 2 8x22B</th>\n",
       "      <td>0.80 ± 0.45</td>\n",
       "      <td>0.80 ± 0.45</td>\n",
       "      <td>0.80 ± 0.45</td>\n",
       "      <td>0.80 ± 0.45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral 7B</th>\n",
       "      <td>0.76 ± 0.36</td>\n",
       "      <td>1.00 ± 0.00</td>\n",
       "      <td>0.76 ± 0.36</td>\n",
       "      <td>0.82 ± 0.29</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral 8x7B</th>\n",
       "      <td>0.45 ± 0.37</td>\n",
       "      <td>0.80 ± 0.45</td>\n",
       "      <td>0.45 ± 0.37</td>\n",
       "      <td>0.55 ± 0.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.74 ± 0.28</td>\n",
       "      <td>0.89 ± 0.14</td>\n",
       "      <td>0.74 ± 0.28</td>\n",
       "      <td>0.78 ± 0.24</td>\n",
       "      <td>21.1</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accuracy    precision       recall           f1  \\\n",
       "model                                                                  \n",
       "Qwen2 72B         1.00 ± 0.00  1.00 ± 0.00  1.00 ± 0.00  1.00 ± 0.00   \n",
       "DeepSeek V3       0.93 ± 0.15  1.00 ± 0.00  0.93 ± 0.15  0.96 ± 0.09   \n",
       "Gemma 2 27b       0.80 ± 0.27  1.00 ± 0.00  0.80 ± 0.27  0.87 ± 0.18   \n",
       "Gemma 2 9b        0.80 ± 0.33  1.00 ± 0.00  0.80 ± 0.33  0.85 ± 0.26   \n",
       "Llama 2 7b        0.19 ± 0.38  0.25 ± 0.50  0.19 ± 0.38  0.21 ± 0.43   \n",
       "Llama 3 70b       0.72 ± 0.31  1.00 ± 0.00  0.72 ± 0.31  0.80 ± 0.25   \n",
       "Llama 3 8b        0.92 ± 0.18  1.00 ± 0.00  0.92 ± 0.18  0.95 ± 0.11   \n",
       "WizardLM 2 8x22B  0.80 ± 0.45  0.80 ± 0.45  0.80 ± 0.45  0.80 ± 0.45   \n",
       "Mistral 7B        0.76 ± 0.36  1.00 ± 0.00  0.76 ± 0.36  0.82 ± 0.29   \n",
       "Mixtral 8x7B      0.45 ± 0.37  0.80 ± 0.45  0.45 ± 0.37  0.55 ± 0.37   \n",
       "Average           0.74 ± 0.28  0.89 ± 0.14  0.74 ± 0.28  0.78 ± 0.24   \n",
       "\n",
       "                  num_valid  num_invalid  \n",
       "model                                     \n",
       "Qwen2 72B              25.0          0.0  \n",
       "DeepSeek V3            22.0          3.0  \n",
       "Gemma 2 27b            21.0          4.0  \n",
       "Gemma 2 9b             21.0          4.0  \n",
       "Llama 2 7b             15.0         10.0  \n",
       "Llama 3 70b            22.0          3.0  \n",
       "Llama 3 8b             25.0          0.0  \n",
       "WizardLM 2 8x22B       15.0         10.0  \n",
       "Mistral 7B             25.0          0.0  \n",
       "Mixtral 8x7B           20.0          5.0  \n",
       "Average                21.1          3.9  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_brewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_single_metric(tables : list[pd.DataFrame], metric : str, labels : list[str] = None):\n",
    "    out = pd.concat(tables, axis=1)\n",
    "    out = out[[metric]]\n",
    "    if labels:\n",
    "        out.columns = labels\n",
    "    out = out.reset_index()\n",
    "    for col in out.columns:\n",
    "        col = col.upper()\n",
    "\n",
    "    return out.reset_index()\n",
    "\n",
    "def as_latex(table):\n",
    "    table = table.drop(columns=['index'])\n",
    "    table = table.rename(columns={'model': 'Model'})\n",
    "    return table.to_latex(index=False, escape=False)\n",
    "\n",
    "joined = join_with_single_metric([pretty_gerrig, pretty_brewer, pretty_delatorre, pretty_lehne], 'f1', labels=['Gerrig', 'Brewer', 'Delatorre', 'Lehne'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model & Gerrig & Brewer & Delatorre & Lehne \\\\\n",
      "\\midrule\n",
      "Qwen2 72B & 1.00 ± 0.00 & 1.00 ± 0.00 & 0.77 ± 0.08 & 0.88 ± 0.0 \\\\\n",
      "DeepSeek V3 & 1.00 ± 0.00 & 0.96 ± 0.09 & 0.79 ± 0.09 & 0.91 ± 0.0 \\\\\n",
      "Gemma 2 27b & 1.00 ± 0.00 & 0.87 ± 0.18 & 0.85 ± 0.04 & 0.92 ± 0.0 \\\\\n",
      "Gemma 2 9b & 1.00 ± 0.00 & 0.85 ± 0.26 & 0.80 ± 0.05 & 0.90 ± 0.0 \\\\\n",
      "Llama 2 7b & 1.00 ± 0.00 & 0.21 ± 0.43 & 0.81 ± 0.04 & 0.91 ± 0.0 \\\\\n",
      "Llama 3 70b & 1.00 ± 0.00 & 0.80 ± 0.25 & 0.79 ± 0.11 & 0.84 ± 0.0 \\\\\n",
      "Llama 3 8b & 1.00 ± 0.00 & 0.95 ± 0.11 & 0.76 ± 0.11 & 0.81 ± 0.0 \\\\\n",
      "WizardLM 2 8x22B & 1.00 ± 0.00 & 0.80 ± 0.45 & 0.80 ± 0.07 & 0.89 ± 0.0 \\\\\n",
      "Mistral 7B & 1.00 ± 0.00 & 0.82 ± 0.29 & 0.79 ± 0.07 & 0.91 ± 0.0 \\\\\n",
      "Mixtral 8x7B & 1.00 ± 0.00 & 0.55 ± 0.37 & 0.80 ± 0.05 & 0.79 ± 0.0 \\\\\n",
      "Average & 1.00 ± 0.00 & 0.78 ± 0.24 & 0.80 ± 0.07 & 0.88 ± 0.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(as_latex(joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>Qwen_Qwen2-72B-Instruct</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>deepseek-ai_DeepSeek-V3</td>\n",
       "      <td>[1, nan, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>google_gemma-2-27b-it</td>\n",
       "      <td>[1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>google_gemma-2-9b-it</td>\n",
       "      <td>[1, nan, 1, 1, 1, 1, 1, 0, 1, nan, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experiment,Journalistic Bad Not Revealed</td>\n",
       "      <td>meta-llama_Llama-2-7b-chat-hf</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>meta-llama_Llama-3-70b-chat-hf</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>meta-llama_Llama-3-8b-chat-hf</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>microsoft_WizardLM-2-8x22B</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>mistralai_Mistral-7B-Instruct-v0.3</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Experiment,Novel Good Revealed</td>\n",
       "      <td>mistralai_Mixtral-8x7B-Instruct-v0.1</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id  \\\n",
       "0   Experiment,Journalistic Bad Not Revealed   \n",
       "1   Experiment,Journalistic Bad Not Revealed   \n",
       "2   Experiment,Journalistic Bad Not Revealed   \n",
       "3   Experiment,Journalistic Bad Not Revealed   \n",
       "4   Experiment,Journalistic Bad Not Revealed   \n",
       "..                                       ...   \n",
       "75            Experiment,Novel Good Revealed   \n",
       "76            Experiment,Novel Good Revealed   \n",
       "77            Experiment,Novel Good Revealed   \n",
       "78            Experiment,Novel Good Revealed   \n",
       "79            Experiment,Novel Good Revealed   \n",
       "\n",
       "                                   model  \\\n",
       "0                Qwen_Qwen2-72B-Instruct   \n",
       "1                deepseek-ai_DeepSeek-V3   \n",
       "2                  google_gemma-2-27b-it   \n",
       "3                   google_gemma-2-9b-it   \n",
       "4          meta-llama_Llama-2-7b-chat-hf   \n",
       "..                                   ...   \n",
       "75        meta-llama_Llama-3-70b-chat-hf   \n",
       "76         meta-llama_Llama-3-8b-chat-hf   \n",
       "77            microsoft_WizardLM-2-8x22B   \n",
       "78    mistralai_Mistral-7B-Instruct-v0.3   \n",
       "79  mistralai_Mixtral-8x7B-Instruct-v0.1   \n",
       "\n",
       "                                    response  \n",
       "0       [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]  \n",
       "1     [1, nan, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "2       [1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "3   [1, nan, 1, 1, 1, 1, 1, 0, 1, nan, 1, 1]  \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "..                                       ...  \n",
       "75      [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]  \n",
       "76      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0]  \n",
       "77      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "78      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "79      [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delatorre_experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thriller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
